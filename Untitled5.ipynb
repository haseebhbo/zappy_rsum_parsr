{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2SDqAORpCNm",
        "outputId": "60708016-f607-4027-fd68-0c8aadcce69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdf2\n",
            "  Downloading PyPDF2-2.10.3-py3-none-any.whl (214 kB)\n",
            "\u001b[K     |████████████████████████████████| 214 kB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pypdf2) (4.1.1)\n",
            "Installing collected packages: pypdf2\n",
            "Successfully installed pypdf2-2.10.3\n"
          ]
        }
      ],
      "source": [
        "pip install pypdf2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "Obj = open('/content/Resume22.PDF', 'rb')"
      ],
      "metadata": {
        "id": "Zrq6EfQxpmhr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = PyPDF2.PdfFileReader(Obj)\n",
        "print(pdf.numPages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQMYLtYcqDme",
        "outputId": "6e0f7551-fcef-4e6c-d5eb-35189d7f838d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pageobj = pdf.getPage(0)\n",
        "print(pageobj.extractText())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwa_bycFqaJ9",
        "outputId": "dc5e2e84-8dc6-4675-e7f6-b42176e498ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HAZEEB O\n",
            "7034432111 \n",
            "Haseebhbo7@gmail.com \n",
            "Ottakath (H) Malappuram kerala \n",
            "OBJECTIVES\n",
            "To obtain employment with a company that o\u0000ers a positive atmosphere to learn and implement new skills and technologies for the\n",
            "betterment of the organization.\n",
            "EXPERIENCE\n",
            "QUALIFICATIONS\n",
            "Technical Skills\n",
            "Machine learning\n",
            "Bigdata: Hadoop\n",
            "Database : MySQL\n",
            "TableauManagement Skills\n",
            "Work ethic\n",
            "Microsoft Excel\n",
            "Statistical analysis and computing\n",
            "Processing large data setsSKILLS\n",
            "PROJECTS\n",
            "ACHIEVEMENTS11/2021 - PresentCochin\n",
            "At Luminar Technolab\n",
            "Analyze large amounts of information to discover trends and patterns, Build predictive models and machine-learning\n",
            "algorithms.\n",
            "7/2016 - 9/2020B Tech computer science engineering\n",
            "From APJ Abdul kalam University\n",
            "6/2014 - 3/2016Plus Two\n",
            "From HSE\n",
            "6/2011 - 8/2012SSLC\n",
            "From Kerala state board\n",
            "75%\n",
            "80%\n",
            "95%\n",
            "80%\n",
            "10/2021 - PresentFake News Detection using machine learning\n",
            "Natural language processing NLP is used to detect fake news with Python. We took a Fake and True News dataset,\n",
            "implemented a Text cleaning function, T\u0000dfVectorizer, initialized Multinomial Naive Bayes Classi\u0000er, and \u0000t our\n",
            "model. We ended up obtaining an accuracy of 95.31% in magnitude. Preprocessing steps : Cleaning data Convert to\n",
            "lowercase Stop word removal Lemmatization T\u0000dfVectorization FITTING THE MODEL (Machine learning algorithm)\n",
            "8/2021NACTET certi\u0000cation in Data science\n",
            "From Luminar Technolab\n",
            "5/2022Online certi\u0000cation\n",
            "From GREAT LEARNING\n",
            "https://olympus1.mygreatlearning.com/course_certi\u0000cate/ZUZTVBXJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=pageobj.extractText()"
      ],
      "metadata": {
        "id": "dve0fl_3qnqF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        "\n",
        "re.findall(EMAIL_REG, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m03IovSxq2jF",
        "outputId": "8ed90aeb-fa2f-4373-c8ad-27ffa979ae72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aseebhbo7@gmail.com']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " mob = re.compile(r\"\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d\")\n",
        "\n",
        "a=re.findall(mob,text)\n",
        "if len(a)==0:\n",
        "  mob = re.compile(r\"\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d\")\n",
        "  a=re.findall(mob,text)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLLeVDzoq7kq",
        "outputId": "315e3cfc-31e7-407a-ac70-957e96931dc2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['7034432111']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdJDW0JjsT--",
        "outputId": "3f25da91-bc6e-4ca1-abdb-52c5b17cf917"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def cleanResume(resumeText):\n",
        "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
        "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
        "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
        "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
        "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
        "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
        "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
        "    return resumeText\n",
        "\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "\n",
        "  txt=cleanResume(text)"
      ],
      "metadata": {
        "id": "2zVLTR70uLrd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "\n",
        "\n",
        "nltk_results = ne_chunk(pos_tag(word_tokenize(txt)))\n",
        "for nltk_result in nltk_results:\n",
        "    if type(nltk_result) == Tree:\n",
        "        name = ''\n",
        "        for nltk_result_leaf in nltk_result.leaves():\n",
        "            name += nltk_result_leaf[0] + ' '\n",
        "        print ( 'Name: ', name)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3buGBX-Vs7mA",
        "outputId": "705fdea8-9bf6-4c59-fbdc-c8e95ff9fc95"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name:  HAZEEB \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su8NkexXG61C",
        "outputId": "79494868-3fac-47c5-ad26-db09e216d1ca"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=479e2f36c6b6c7b83ce8ebf5d939bf2eba36ba40104f9795dbe07b8f45dead3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt\n",
        "import nltk\n",
        " \n",
        "nltk.download('stopwords')\n",
        " \n",
        "# you may read the database from a csv file or some other database\n",
        "SKILLS_DB = [\n",
        "    'machine learning',\n",
        "    'data science',\n",
        "    'python',\n",
        "    'word',\n",
        "    'excel',\n",
        "    'English',\n",
        "]\n",
        " \n",
        " \n",
        "def extract_text_from_docx(docx_path):\n",
        "    txt = docx2txt.process(docx_path)\n",
        "    if txt:\n",
        "        return txt.replace('\\t', ' ')\n",
        "    return None\n",
        " \n",
        " \n",
        "def extract_skills(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        " \n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        " \n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        " \n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        " \n",
        "    # we create a set to keep the results in.\n",
        "    found_skills = set()\n",
        " \n",
        "    # we search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() in SKILLS_DB:\n",
        "            found_skills.add(token)\n",
        " \n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in SKILLS_DB:\n",
        "            found_skills.add(ngram)\n",
        " \n",
        "    return found_skills\n",
        " \n",
        " \n",
        "if __name__ == '__main__':\n",
        "    text = extract_text_from_docx('/content/Resume22.docx')\n",
        "    skills = extract_skills(text)\n",
        "    print(skills)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK_3zQR243G3",
        "outputId": "cc2f3b6e-dae9-4ef2-871d-1615c2d6e5b5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Python', 'word', 'Machine learning', 'machine learning', 'Data science', 'Excel'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AelDwaq-Gjh3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}